{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set API key\n",
    "client = OpenAI(api_key = os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model='text-embedding-3-small'):\n",
    "    if not text.strip():\n",
    "        text = \"No content available on this page.\"\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "\n",
    "def chunk_page_texts(page_texts, chunk_size=300, chunk_overlap=50):\n",
    "    chunks = []\n",
    "    for page_num, text in enumerate(page_texts):\n",
    "        words = text.split()\n",
    "        start = 0\n",
    "        while start < len(words):\n",
    "            end = start + chunk_size\n",
    "            chunk = words[start:end]\n",
    "            chunk_text = \" \".join(chunk)\n",
    "            chunks.append({\n",
    "                \"page\": page_num + 1,\n",
    "                \"chunk_text\": chunk_text\n",
    "            })\n",
    "            start += chunk_size - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "def get_similarities(query_text, rag_db, top_x=3):\n",
    "    # Select the text to be queried\n",
    "    query_embedding = get_embedding(query_text)\n",
    "\n",
    "    # Compute similarities as a list of tuples\n",
    "    similarities = [\n",
    "        (row['embedding'], cosine_similarity([query_embedding], [row['embedding']])[0][0], row['chunk_text'])\n",
    "        for _, row in rag_db.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Sort by similarity (second element in the tuple) in descending order\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Retrieve top x results\n",
    "    similarity_text = []\n",
    "    for i in range(top_x):\n",
    "        similarity_text.append(similarities[i][2])\n",
    "\n",
    "    context_str = \"\\n\\n\".join(similarity_text)\n",
    "    \n",
    "    return context_str\n",
    "\n",
    "def llm_response(question, df):\n",
    "    # Fetch similarities\n",
    "    top_k = 5\n",
    "    context_str = get_similarities(question, df, top_k)\n",
    "\n",
    "    # Create llm prompt\n",
    "    prompt = create_prompt(question, context_str)\n",
    "\n",
    "    # Call the LLM (e.g., GPT-4 or GPT-3.5)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",  # or \"gpt-3.5-turbo\"\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # Return LLM response\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def create_prompt(question, content):\n",
    "    prompt = f\"\"\"\n",
    "    You are a Chartered Finanacial Analyst. Answer the User Question based on the provided Context.\n",
    "    \n",
    "    User Question:\n",
    "    {question}\n",
    "    \n",
    "    Context:\n",
    "    {content}\n",
    "    \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DF\n",
    "df = pd.read_json(\"tsla_chunks.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla is a company with a strong focus on corporate governance and business ethics. They have established an ESG Sustainability Council consisting of leaders from across their company, and maintain a worldwide employee headcount of 140,473 as of December 31, 2023. Tesla has a competitive edge in attracting and retaining high-quality employees, and they have an environment that fosters growth opportunities, with nearly two-thirds (65%) of their managers being promoted from internal, non-manager positions. \n",
      "\n",
      "Tesla has grown by 35% over the past two years, offering career development and meaningful contributions to a sustainable future. They retain employees by providing excellent health benefits, stock ownership opportunities, and continuous development training for leaders. \n",
      "\n",
      "Intellectual property is a priority for Tesla, with a focus on innovative approach and proprietary designs. They protect their intellectual property rights through patents, trademarks, copyrights, trade secrets, and confidentiality agreements. \n",
      "\n",
      "Tesla's mission is to accelerate the world's transition to sustainable energy, with a focus on reducing carbon emissions through energy generation and consumption. They have been making strides in automation, die-making, and line-building, and are also focusing on energy generation and storage. \n",
      "\n",
      "Tesla uses a direct sales method for their vehicles, through their website and international network of company-owned stores. They believe this helps to control inventory costs, manage warranty service and pricing, educate consumers, and obtain customer feedback. \n",
      "\n",
      "However, Tesla acknowledges the risk of dependency on key employees, including Elon Musk, and the competitive labor market. They have also received several requests for information from government agencies on topics such as operations, technology, compliance, and finance, but to their knowledge, no wrongdoing has been found.\n"
     ]
    }
   ],
   "source": [
    "# Ask the LLM a question\n",
    "\n",
    "question = \"What is the name of the company that filed the document?\"\n",
    "# question = \"What else do you know about the company that filed the document?\"\n",
    "#question = \"What fiscal years are included the reporting period for the financial statements?\"\n",
    "#question = \"How has the companyâ€™s revenue mix changed across segments or geographies compared to the prior year?\"\n",
    "#question = \"What forward-looking statements or guidance does the company provide about future performance or strategy?\"\n",
    "#question = \"How does the company describe its competitive advantages or market position in the Business section?\"\n",
    "#question = \"Does the company disclose any material weaknesses in internal controls over financial reporting?\"\n",
    "#question = \"What are the most significant risk factors identified by the company?\"\n",
    "\n",
    "print(llm_response(question, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_demo)",
   "language": "python",
   "name": "rag_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
